{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a678d4c",
      "metadata": {
        "id": "4a678d4c"
      },
      "source": [
        "# Install Dependencies\n",
        "We need these libraries to run the LLM and the web server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035a5e07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "035a5e07",
        "outputId": "d89d8cec-92bc-474d-d941-5226206c3a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.9.0+cu126\n",
            "Uninstalling torch-2.9.0+cu126:\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install -q -U transformers accelerate bitsandbytes flask flask-cors pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f7f4326",
      "metadata": {
        "id": "2f7f4326"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"TorchVision:\", torchvision.__version__)\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed1839a8",
      "metadata": {
        "id": "ed1839a8"
      },
      "source": [
        "# Load the SLM (Qwen2.5-Coder-7B)\n",
        "- Qwen2.5-Coder is currently arguably the best \"Small\" model for code & logic.\n",
        "- We load it in 4-bit mode to fit inside the free 15GB GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d187f3a",
      "metadata": {
        "id": "0d187f3a"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "\n",
        "print(\"‚è≥ Loading Model... this takes about 2-3 minutes...\")\n",
        "\n",
        "# Quantization Config (Makes the model smaller)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Load Tokenizer & Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model Loaded Successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1d7a35",
      "metadata": {
        "id": "0f1d7a35"
      },
      "source": [
        "# Start the API Server (Flask + Ngrok)\n",
        "- Sign up at ngrok.com for free and get your auth token\n",
        "- Replace 'YOUR_NGROK_TOKEN' below, or the tunnel might disconnect quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13cbc336",
      "metadata": {
        "id": "13cbc336"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    print(\"‚ö†Ô∏è NGROK_AUTH_TOKEN not found in .env.local\")\n",
        "else:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Allow your local Next.js app to hit this"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2087a41",
      "metadata": {
        "id": "c2087a41"
      },
      "source": [
        "# SYSTEM PROMPT (Enforcing JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f5f649c",
      "metadata": {
        "id": "4f5f649c"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a Python Data Analyst.\n",
        "Your goal is to answer the user's question by writing a VALID PYTHON SCRIPT.\n",
        "\n",
        "RULES:\n",
        "1. You have a pandas DataFrame named 'df' ALREADY LOADED. Do not load it yourself.\n",
        "2. You MUST use 'df' to calculate the answer.\n",
        "3. The LAST line of your script must print a JSON object.\n",
        "4. Do NOT wrap code in markdown blocks (like ```python). Just raw code.\n",
        "5. If the user asks for a chart, return the 'chart' JSON type.\n",
        "6. If the user asks for a table, return the 'table' JSON type.\n",
        "\n",
        "EXPECTED JSON OUTPUT STRUCTURE:\n",
        "type ChartPayload = {\n",
        "  config: {\n",
        "    type: 'bar' | 'line' | 'pie';\n",
        "    title: string;\n",
        "    xAxisKey: string;\n",
        "    series: { dataKey: string; label: string; color?: string }[];\n",
        "  };\n",
        "  data: any[];\n",
        "};\n",
        "\n",
        "type Output =\n",
        "  | { type: 'markdown'; summary: string }\n",
        "  | { type: 'chart'; summary: string; data: ChartPayload }\n",
        "  | { type: 'table'; summary: string; data: { headers: string[]; rows: any[][] } }\n",
        "  | { type: 'kpi'; summary: string; data: { label: string; value: string; status?: 'positive'|'negative' }[] };\n",
        "\n",
        "EXAMPLE PYTHON SCRIPT:\n",
        "monthly = df.groupby('Month')['Revenue'].sum().reset_index()\n",
        "print(json.dumps({\n",
        "  \"type\": \"chart\",\n",
        "  \"summary\": \"Revenue peaked in December.\",\n",
        "  \"data\": {\n",
        "    \"config\": { \"type\": \"bar\", \"title\": \"Revenue\", \"xAxisKey\": \"Month\", \"series\": [{\"dataKey\": \"Revenue\", \"label\": \"Rev\"}] },\n",
        "    \"data\": monthly.to_dict(orient='records')\n",
        "  }\n",
        "}))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a64bf93",
      "metadata": {
        "id": "8a64bf93"
      },
      "source": [
        "# Creating Custom LLM Endpoint Request\n",
        "- We send both the user prompt and the system prompt to the custom LLM endpoint.\n",
        "- This ensures the model understands its role and the expected output format.\n",
        "- We run the custom endpoint and expose it via ngrok for easy access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a551397",
      "metadata": {
        "id": "4a551397"
      },
      "outputs": [],
      "source": [
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    data = request.json\n",
        "    user_prompt = data.get('prompt', '')\n",
        "    system_prompt = data.get('systemPrompt', SYSTEM_PROMPT)\n",
        "\n",
        "    if not user_prompt:\n",
        "        return jsonify({'error': 'No prompt provided'}), 400\n",
        "\n",
        "    print(f\"üì© Received Request: {user_prompt[:50]}...\")\n",
        "\n",
        "    # Combine System Prompt + User Prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Apply Chat Template (Handles the internal formatting for the model)\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate Response\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=1024, # Allow enough space for code\n",
        "        temperature=0.1,     # Low temp for precise code\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Decode and clean up\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return jsonify({'text': response_text})\n",
        "\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(f\"üöÄ API is LIVE at: {public_url}\")\n",
        "print(f\"‚ö†Ô∏è Copy this URL into your .env.local as CUSTOM_LLM_URL\")\n",
        "\n",
        "# Run Flask\n",
        "app.run(port=5000)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}