{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLt8wIatjH7q"
      },
      "outputs": [],
      "source": [
        "# Step 0: Setup\n",
        "!apt-get update -y\n",
        "!apt-get install -y git git-lfs build-essential cmake python3-pip\n",
        "!git lfs install\n",
        "\n",
        "# Step 1: Clone & build llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!cmake -B build\n",
        "!cmake --build build -j 1\n",
        "\n",
        "# Step 2: Download Qwen2.5-Coder-3B-Instruct\n",
        "%cd /content\n",
        "!git clone https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct\n",
        "\n",
        "# Step 3: Convert HF → GGUF (F16)\n",
        "%cd /content/llama.cpp\n",
        "!python3 convert_hf_to_gguf.py /content/Qwen2.5-Coder-3B-Instruct \\\n",
        "  --outfile /content/qwen2.5-coder-3b-instruct-f16.gguf\n",
        "\n",
        "# Step 4: Quantize GGUF → Q4_K_M\n",
        "!./build/bin/llama-quantize \\\n",
        "  /content/qwen2.5-coder-3b-instruct-f16.gguf \\\n",
        "  /content/qwen2.5-coder-3b-instruct-q4_k_m.gguf \\\n",
        "  Q4_K_M\n",
        "\n",
        "# Step 5: Check size\n",
        "!ls -lh /content/*.gguf\n",
        "\n",
        "# Step 6: Download\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/qwen2.5-coder-3b-instruct-q4_k_m.gguf\")\n",
        "\n",
        "# Donwload from the /content\n",
        "\n",
        "print(\"This is the end\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Reset\n",
        "!rm -f /content/qwen2.5-coder-3b-instruct-q4_k_m.gguf\n",
        "# !rm -rf /content"
      ],
      "metadata": {
        "id": "YVBVZKAGomq8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the f16 file\n",
        "# It should be around 6.2G\n",
        "!ls -lh /content/qwen2.5-coder-3b-instruct-f16.gguf\n",
        "\n",
        "# Verify the quantized model\n",
        "# This should be like ~2GB\n",
        "!ls -lh /content/qwen2.5-coder-3b-instruct-q4_k_m.gguf\n"
      ],
      "metadata": {
        "id": "0hEKwIj5on5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "! /content/llama.cpp/build/bin/llama-cli \\\n",
        "  -m /content/qwen2.5-coder-3b-instruct-q4_k_m.gguf \\\n",
        "  -p \"Write python code to print hello world\" \\\n",
        "  -n 40\n"
      ],
      "metadata": {
        "id": "rBMqeKK-pwK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}